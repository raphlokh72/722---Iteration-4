{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387a1285",
   "metadata": {},
   "source": [
    "# 01 - Business Understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6168ff0b",
   "metadata": {},
   "source": [
    "Using Data Mining the aim is to idenity and predict the factors \n",
    "which affect the use of bike-sharing rental services "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fcd3e7",
   "metadata": {},
   "source": [
    "# 02 -Data Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68875c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/24 07:15:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('train.csv')\n",
    "\n",
    "df_pd = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n",
    "\n",
    "df.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns \n",
    "df = df.withColumnRenamed('_c0', 'datetime')\n",
    "df = df.withColumnRenamed('_c1', 'season')\n",
    "df = df.withColumnRenamed('_c2', 'holiday')\n",
    "df = df.withColumnRenamed('_c3', 'workday')\n",
    "df = df.withColumnRenamed('_c4', 'weather')\n",
    "df = df.withColumnRenamed('_c5', 'temp')\n",
    "df = df.withColumnRenamed('_c6', 'feels_temp')\n",
    "df = df.withColumnRenamed('_c7', 'humidity')\n",
    "df = df.withColumnRenamed('_c8', 'windspeed')\n",
    "df = df.withColumnRenamed('_c9', 'casual')\n",
    "df = df.withColumnRenamed('_c10', 'registered')\n",
    "df = df.withColumnRenamed('_c11', 'count')\n",
    "\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1965e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out initial row as it is column lable \n",
    "df = df.filter(df.datetime!='datetime')\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a77c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Dataframe Components Summary \n",
    "\n",
    "df.select(\"datetime\", \"season\", 'holiday', 'workday').summary().show()\n",
    "df.select('weather',\"temp\", \"feels_temp\", 'humidity', 'windspeed').summary().show()\n",
    "df.select(\"casual\", \"registered\", 'count',).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a436005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "\n",
    "df_pd.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5555d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are any null or missing values\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "null_counts = df.select([col(column).isNull().alias(column) for column in df.columns]).toPandas().sum()\n",
    "print(\"Null value counts in DataFrame:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440bb0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('count',).summary().show()\n",
    "df_pd['count'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea7a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd['season'].value_counts().plot.pie(autopct='%1.0f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd['weather'].value_counts().plot.pie(autopct='%1.0f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d69e3",
   "metadata": {},
   "source": [
    "# 03 - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in the relevant types.\n",
    "from pyspark.sql.types import (StructField,StringType,IntegerType,StructType)\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3047543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split datetime varaible into Year, Month, Day and Hour\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "split_col = split(df['datetime'], '-')\n",
    "\n",
    "df = df.withColumn('Year', split_col.getItem(0))\n",
    "df = df.withColumn('Month', split_col.getItem(1))\n",
    "df = df.withColumn('Day_Time', split_col.getItem(2))\n",
    "\n",
    "\n",
    "split_col = split(df['Day_Time'], '\\s+')\n",
    "df = df.withColumn('Day', split_col.getItem(0))\n",
    "df = df.withColumn('Time', split_col.getItem(1))\n",
    "\n",
    "split_col = split(df['Time'], ':')\n",
    "df = df.withColumn('Hour', split_col.getItem(0))\n",
    "\n",
    "df = df.drop('datetime')\n",
    "df = df.drop('Day_Time')\n",
    "df = df.drop('Time')\n",
    "df.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reformt Schema\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, BooleanType\n",
    "\n",
    "df = df.withColumn(\"season\" ,df[\"season\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"holiday\" ,df[\"holiday\"].cast(BooleanType()))\n",
    "df = df.withColumn(\"workday\" ,df[\"workday\"].cast(BooleanType()))\n",
    "df = df.withColumn(\"weather\" ,df[\"weather\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"temp\" ,df[\"temp\"].cast(FloatType()))\n",
    "df = df.withColumn(\"feels_temp\" ,df[\"feels_temp\"].cast(FloatType()))\n",
    "df = df.withColumn(\"humidity\" ,df[\"humidity\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"windspeed\" ,df[\"windspeed\"].cast(FloatType()))\n",
    "df = df.withColumn(\"casual\" ,df[\"casual\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"registered\" ,df[\"registered\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"count\" ,df[\"count\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"Year\" ,df[\"Year\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"Month\" ,df[\"Month\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"Day\" ,df[\"Day\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"Hour\" ,df[\"Hour\"].cast(IntegerType()))\n",
    "\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd49c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat Season and Weather Variables \n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_reformated = df.select(\"*\") \n",
    "\n",
    "df_reformated = df_reformated.withColumn(\"season\",\n",
    "                   when(df_reformated[\"season\"] == \"1\", \"Spring\")\n",
    "                   .when(df_reformated[\"season\"] == \"2\", \"Summer\")\n",
    "                   .when(df_reformated[\"season\"] == \"3\", \"Autumn\")\n",
    "                   .when(df_reformated[\"season\"] == \"4\", \"Winter\")\n",
    "                   .otherwise(df_reformated[\"season\"]))\n",
    "\n",
    "df_reformated = df_reformated.withColumn(\"weather\",\n",
    "                   when(df_reformated[\"weather\"] == \"1\", \"Sunny\")\n",
    "                   .when(df_reformated[\"weather\"] == \"2\", \"Cloudy\")\n",
    "                   .when(df_reformated[\"weather\"] == \"3\", \"Lite R/S\")\n",
    "                   .when(df_reformated[\"weather\"] == \"4\", \"Severe Con\")\n",
    "                   .otherwise(df_reformated[\"weather\"]))\n",
    "\n",
    "df.show()\n",
    "df_reformated.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"season\", \"holiday\", 'workday').summary().show()\n",
    "df.select('weather',\"temp\", \"feels_temp\", 'humidity', 'windspeed').summary().show()\n",
    "df.select(\"casual\", \"registered\", 'count',).summary().show()\n",
    "df.select(\"Year\", \"Month\", 'Day','Hour').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation\n",
    "\n",
    "#dealing with outliers in humidity and windspeed variables \n",
    "\n",
    "from pyspark.sql.functions import col, mean, stddev\n",
    "\n",
    "# Calculate mean and std deviation for the 'humidity' column\n",
    "stats = df.select(mean(col(\"humidity\")).alias('mean_humidity'), \n",
    "                  stddev(col(\"humidity\")).alias('stddev_humidity')).collect()\n",
    "\n",
    "# Extract mean and std deviation values\n",
    "mean_val = stats[0]['mean_humidity']\n",
    "stddev_val = stats[0]['stddev_humidity']\n",
    "\n",
    "# Define threshold for outliers (e.g., 3 standard deviations)\n",
    "threshold = 3\n",
    "\n",
    "# Filter out outliers based on the Z-score\n",
    "df = df.filter((col(\"humidity\") > mean_val - threshold * stddev_val) & \n",
    "                            (col(\"humidity\") < mean_val + threshold * stddev_val))\n",
    "\n",
    "\n",
    "stats = df.select(mean(col(\"windspeed\")).alias('mean_windspeed'), \n",
    "                    stddev(col(\"windspeed\")).alias('stddev_windspeed')).collect()\n",
    "\n",
    "# Extract mean and std deviation values\n",
    "mean_val = stats[0]['mean_windspeed']\n",
    "stddev_val = stats[0]['stddev_windspeed']\n",
    "\n",
    "# Define threshold for outliers\n",
    "threshold = 3.0  # Example threshold as a float\n",
    "\n",
    "# Filter out outliers based on the Z-score using the threshold\n",
    "df = df.filter((col(\"windspeed\") > mean_val - threshold * stddev_val) & \n",
    "                            (col(\"windspeed\") < mean_val + threshold * stddev_val))\n",
    "\n",
    "\n",
    "# Show the filtered data\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c493bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"season\", \"holiday\", 'workday').summary().show()\n",
    "df.select('weather',\"temp\", \"feels_temp\", 'humidity', 'windspeed').summary().show()\n",
    "df.select(\"casual\", \"registered\", 'count',).summary().show()\n",
    "df.select(\"Year\", \"Month\", 'Day','Hour').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a885638a",
   "metadata": {},
   "source": [
    "# 04 - Data Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Reduction\n",
    "#Convert dataframe to Pandas to view correlation matirx\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "correlation_matrix = df_pd.corr()\n",
    "\n",
    "# Print correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix['count'])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Varaibles that are seen as to not have a high correlations with target variable \"count\"\n",
    "df.show()\n",
    "\n",
    "df = df.drop('holiday', 'workday', 'weather', 'humidity', 'Day')\n",
    "df_reformated = df_reformated.drop('holiday', 'workday', 'weather', 'humidity', 'Day')\n",
    "\n",
    "df.show()\n",
    "df_reformated.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e31475",
   "metadata": {},
   "source": [
    "# 05 - Data Mining Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b17313b",
   "metadata": {},
   "source": [
    "### Classification Methods \n",
    "- Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88783b",
   "metadata": {},
   "source": [
    "### Clustering Methods \n",
    "- K Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4127e",
   "metadata": {},
   "source": [
    "### Regression Methods\n",
    "- Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4b85b",
   "metadata": {},
   "source": [
    "#### Tree Methods\n",
    "- Single Decision Tree\n",
    "- Random Forest \n",
    "- Gradient Boosted ree Classifer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23256ffc",
   "metadata": {},
   "source": [
    "# 06 - Data Mining Method Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde909f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e79a3a52",
   "metadata": {},
   "source": [
    "# 07 - Data Mining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e93d1",
   "metadata": {},
   "source": [
    "### K-MEANS CLUSTERING\n",
    "#### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f2883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "df.head(1)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81603f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn(\"count\", df[\"count\"].cast(IntegerType()))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed967341",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = ['season', 'temp', 'feels_temp', 'windspeed', 'casual', 'registered', 'count', 'Year', 'Month', 'Hour']\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')\n",
    "\n",
    "final_data = vec_assembler.transform(df)\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b4ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerModel = scaler.fit(final_data)\n",
    "cluster_final_data = scalerModel.transform(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d68cca",
   "metadata": {},
   "source": [
    "### Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans3 = KMeans(featuresCol='scaledFeatures',k=3)\n",
    "kmeans2 = KMeans(featuresCol='scaledFeatures',k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1586275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = kmeans3.fit(cluster_final_data)\n",
    "model2 = kmeans2.fit(cluster_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Make predictions\n",
    "predictions3 = model3.transform(cluster_final_data)\n",
    "predictions2 = model2.transform(cluster_final_data)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette3 = evaluator.evaluate(predictions3)\n",
    "print(\"With k=3 Silhouette with squared euclidean distance = \" + str(silhouette3))\n",
    "\n",
    "silhouette2 = evaluator.evaluate(predictions2)\n",
    "print(\"With k=2 Silhouette with squared euclidean distance = \" + str(silhouette2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeadbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESULTS \n",
    "centers3 = model3.clusterCenters()\n",
    "print(\"Cluster Centers for k=3:\")\n",
    "for center in centers3:\n",
    "    print(center)\n",
    "\n",
    "centers2 = model2.clusterCenters()\n",
    "print(\"Cluster Centers for k=2:\")\n",
    "for center in centers2:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "for k in range(2,9):\n",
    "    kmeans = KMeans(featuresCol='scaledFeatures',k=k)\n",
    "    model = kmeans.fit(cluster_final_data)\n",
    "    predictions = model.transform(cluster_final_data)\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    print(\"With K={}\".format(k))\n",
    "    print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "    print('--'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b424574",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c404ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9adfb5",
   "metadata": {},
   "source": [
    "### Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f17eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cols = feat_cols + ['prediction']\n",
    "\n",
    "pdf3 = predictions3.select(viz_cols).toPandas()\n",
    "pdf2 = predictions2.select(viz_cols).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae29498",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers3_df = pd.DataFrame(model3.clusterCenters(), columns=feat_cols)\n",
    "centers2_df = pd.DataFrame(model2.clusterCenters(), columns=feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9852c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(data, centers, k, feature):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=feature, y='count', hue='prediction', data=data, palette='viridis')\n",
    "    plt.scatter(centers[feature], centers['count'], s=300, c='red', marker='X')\n",
    "    plt.title(f'K-means Clustering (k={k}) - {feature} vs Count')\n",
    "    plt.xlabel(feature.capitalize())\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e9db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clusters for k=3\n",
    "for feature in feat_cols:\n",
    "    if feature != 'count':\n",
    "        plot_clusters(pdf3, centers3_df, 3, feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61889198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clusters for k=2\n",
    "for feature in feat_cols:\n",
    "    if feature != 'count':\n",
    "        plot_clusters(pdf2, centers2_df, 2, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf793e",
   "metadata": {},
   "source": [
    "#### Python Sklearn Compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a366b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ae7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl = df.toPandas()\n",
    "\n",
    "df_cl.info()\n",
    "df_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing over the standard deviation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df_cl.values[:, 1:]\n",
    "\n",
    "X = np.nan_to_num(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterNum = 3\n",
    "k_means = KMeans(init=\"k-means++\", n_clusters=clusterNum, n_init=12)\n",
    "k_means.fit(X_normalized)\n",
    "labels = k_means.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78929a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl[\"Cluster\"] = labels\n",
    "\n",
    "df_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b558aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl.groupby('Cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce835e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_cl, hue='Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_cl, x=\"count\", y='temp',hue=\"Cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e93f6",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "#### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed73835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('lr_example').getOrCreate()\n",
    "\n",
    "data = df\n",
    "\n",
    "# Show the data\n",
    "data.show()\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea01d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input columns and assembler\n",
    "input_cols = [\"season\", \"temp\", \"feels_temp\", \"windspeed\", \"casual\", \"registered\", \"Year\", \"Month\", \"Hour\"]\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "\n",
    "\n",
    "output = assembler.transform(data)\n",
    "output.select(\"features\").show(3)\n",
    "\n",
    "output.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select(\"features\", \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print schema and show final data\n",
    "final_data.printSchema()\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac9232",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to training and test: \n",
    "\n",
    "train, test = final_data.randomSplit([0.7,0.3])\n",
    "\n",
    "train.describe().show()\n",
    "test.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecaa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol='count')\n",
    "\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc88724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: {}\".format(str(lrModel.coefficients)))\n",
    "print(\"Intercept: {}\".format(str(lrModel.intercept)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6647efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_results = lrModel.evaluate(test)\n",
    "test_results.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print RMSE and R2 values\n",
    "print(\"RMSE: {}\".format(test_results.rootMeanSquaredError))\n",
    "print(\"R2: {}\".format(test_results.r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the final data\n",
    "final_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f3e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-Validation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define the linear regression model\n",
    "lr = LinearRegression(labelCol='count')\n",
    "\n",
    "# Create a parameter grid for cross-validation (if needed)\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "# Define the cross-validator\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol='count'),\n",
    "                          numFolds=5)  # Use 5-fold cross-validation\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(final_data)\n",
    "\n",
    "# Get the best model from cross-validation\n",
    "bestModel = cvModel.bestModel\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "test_results_cv = bestModel.evaluate(test)\n",
    "print(\"Cross-Validated RMSE: {}\".format(test_results_cv.rootMeanSquaredError))\n",
    "print(\"Cross-Validated R2: {}\".format(test_results_cv.r2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc05cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect Model Results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = bestModel.transform(test)\n",
    "\n",
    "# Collect predicted and actual values to the local environment for plotting\n",
    "predictions_pd = predictions.select(\"prediction\", \"count\").toPandas()\n",
    "\n",
    "# Plot predicted vs. actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=predictions_pd['count'], y=predictions_pd['prediction'])\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predicted vs Actual Values')\n",
    "plt.plot([predictions_pd['count'].min(), predictions_pd['count'].max()],\n",
    "         [predictions_pd['count'].min(), predictions_pd['count'].max()],\n",
    "         color='red', linestyle='--')  # Line y=x for reference\n",
    "plt.show()\n",
    "\n",
    "# Residuals Plot\n",
    "residuals = predictions_pd['count'] - predictions_pd['prediction']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, bins=30)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residuals Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs. Fitted Values Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=predictions_pd['prediction'], y=residuals)\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Fitted Values')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Q-Q Plot of Residuals\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sm.qqplot(np.array(residuals), line='45')\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb00a2",
   "metadata": {},
   "source": [
    "# 08 - Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88523ca",
   "metadata": {},
   "source": [
    "#### K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa0770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a86375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c870be97",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb03648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61461716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22276b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd75f152",
   "metadata": {},
   "source": [
    "Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc97d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
